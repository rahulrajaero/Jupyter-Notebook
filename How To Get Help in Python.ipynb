{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook desribes how to get help in ```python```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python 3.7's help utility!\n",
      "\n",
      "If this is your first time using Python, you should definitely check out\n",
      "the tutorial on the Internet at https://docs.python.org/3.7/tutorial/.\n",
      "\n",
      "Enter the name of any module, keyword, or topic to get help on writing\n",
      "Python programs and using Python modules.  To quit this help utility and\n",
      "return to the interpreter, just type \"quit\".\n",
      "\n",
      "To get a list of available modules, keywords, symbols, or topics, type\n",
      "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
      "with a one-line summary of what it does; to list the modules whose name\n",
      "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
      "\n",
      "help> collections\n",
      "Help on package collections:\n",
      "\n",
      "NAME\n",
      "    collections\n",
      "\n",
      "DESCRIPTION\n",
      "    This module implements specialized container datatypes providing\n",
      "    alternatives to Python's general purpose built-in containers, dict,\n",
      "    list, set, and tuple.\n",
      "    \n",
      "    * namedtuple   factory function for creating tuple subclasses with named fields\n",
      "    * deque        list-like container with fast appends and pops on either end\n",
      "    * ChainMap     dict-like class for creating a single view of multiple mappings\n",
      "    * Counter      dict subclass for counting hashable objects\n",
      "    * OrderedDict  dict subclass that remembers the order entries were added\n",
      "    * defaultdict  dict subclass that calls a factory function to supply missing values\n",
      "    * UserDict     wrapper around dictionary objects for easier dict subclassing\n",
      "    * UserList     wrapper around list objects for easier list subclassing\n",
      "    * UserString   wrapper around string objects for easier string subclassing\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    abc\n",
      "\n",
      "SUBMODULES\n",
      "    _collections_abc\n",
      "\n",
      "CLASSES\n",
      "    builtins.dict(builtins.object)\n",
      "        Counter\n",
      "        OrderedDict\n",
      "        defaultdict\n",
      "    builtins.object\n",
      "        deque\n",
      "    collections.abc.MutableMapping(collections.abc.Mapping)\n",
      "        ChainMap\n",
      "        UserDict\n",
      "    collections.abc.MutableSequence(collections.abc.Sequence)\n",
      "        UserList\n",
      "    collections.abc.Sequence(collections.abc.Reversible, collections.abc.Collection)\n",
      "        UserString\n",
      "    \n",
      "    class ChainMap(collections.abc.MutableMapping)\n",
      "     |  ChainMap(*maps)\n",
      "     |  \n",
      "     |  A ChainMap groups multiple dicts (or other mappings) together\n",
      "     |  to create a single, updateable view.\n",
      "     |  \n",
      "     |  The underlying mappings are stored in a list.  That list is public and can\n",
      "     |  be accessed or updated using the *maps* attribute.  There is no other\n",
      "     |  state.\n",
      "     |  \n",
      "     |  Lookups search the underlying mappings successively until a key is found.\n",
      "     |  In contrast, writes, updates, and deletions only operate on the first\n",
      "     |  mapping.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChainMap\n",
      "     |      collections.abc.MutableMapping\n",
      "     |      collections.abc.Mapping\n",
      "     |      collections.abc.Collection\n",
      "     |      collections.abc.Sized\n",
      "     |      collections.abc.Iterable\n",
      "     |      collections.abc.Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __bool__(self)\n",
      "     |  \n",
      "     |  __contains__(self, key)\n",
      "     |  \n",
      "     |  __copy__ = copy(self)\n",
      "     |  \n",
      "     |  __delitem__(self, key)\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |  \n",
      "     |  __init__(self, *maps)\n",
      "     |      Initialize a ChainMap by setting *maps* to the given mappings.\n",
      "     |      If no mappings are provided, a single empty dictionary is used.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __missing__(self, key)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      Clear maps[0], leaving maps[1:] intact.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      New ChainMap or subclass with a new copy of maps[0] and refs to maps[1:]\n",
      "     |  \n",
      "     |  get(self, key, default=None)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  new_child(self, m=None)\n",
      "     |      New ChainMap with a new map followed by all previous maps.\n",
      "     |      If no map is provided, an empty dict is used.\n",
      "     |  \n",
      "     |  pop(self, key, *args)\n",
      "     |      Remove *key* from maps[0] and return its value. Raise KeyError if *key* not in maps[0].\n",
      "     |  \n",
      "     |  popitem(self)\n",
      "     |      Remove and return an item pair from maps[0]. Raise KeyError is maps[0] is empty.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromkeys(iterable, *args) from abc.ABCMeta\n",
      "     |      Create a ChainMap with a single dict created from the iterable.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  parents\n",
      "     |      New ChainMap from maps[1:].\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.MutableMapping:\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None)\n",
      "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      "     |  \n",
      "     |  update(*args, **kwds)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
      "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k, v in F.items(): D[k] = v\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.Mapping:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  items(self)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(self)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  values(self)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from collections.abc.Mapping:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __reversed__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.abc.Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class Counter(builtins.dict)\n",
      "     |  Counter(*args, **kwds)\n",
      "     |  \n",
      "     |  Dict subclass for counting hashable items.  Sometimes called a bag\n",
      "     |  or multiset.  Elements are stored as dictionary keys and their counts\n",
      "     |  are stored as dictionary values.\n",
      "     |  \n",
      "     |  >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n",
      "     |  \n",
      "     |  >>> c.most_common(3)                # three most common elements\n",
      "     |  [('a', 5), ('b', 4), ('c', 3)]\n",
      "     |  >>> sorted(c)                       # list all unique elements\n",
      "     |  ['a', 'b', 'c', 'd', 'e']\n",
      "     |  >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n",
      "     |  'aaaaabbbbcccdde'\n",
      "     |  >>> sum(c.values())                 # total of all counts\n",
      "     |  15\n",
      "     |  \n",
      "     |  >>> c['a']                          # count of letter 'a'\n",
      "     |  5\n",
      "     |  >>> for elem in 'shazam':           # update counts from an iterable\n",
      "     |  ...     c[elem] += 1                # by adding 1 to each element's count\n",
      "     |  >>> c['a']                          # now there are seven 'a'\n",
      "     |  7\n",
      "     |  >>> del c['b']                      # remove all 'b'\n",
      "     |  >>> c['b']                          # now there are zero 'b'\n",
      "     |  0\n",
      "     |  \n",
      "     |  >>> d = Counter('simsalabim')       # make another counter\n",
      "     |  >>> c.update(d)                     # add in the second counter\n",
      "     |  >>> c['a']                          # now there are nine 'a'\n",
      "     |  9\n",
      "     |  \n",
      "     |  >>> c.clear()                       # empty the counter\n",
      "     |  >>> c\n",
      "     |  Counter()\n",
      "     |  \n",
      "     |  Note:  If a count is set to zero or reduced to zero, it will remain\n",
      "     |  in the counter until the entry is deleted or the counter is cleared:\n",
      "     |  \n",
      "     |  >>> c = Counter('aaabbc')\n",
      "     |  >>> c['b'] -= 2                     # reduce the count of 'b' by two\n",
      "     |  >>> c.most_common()                 # 'b' is still in, but its count is zero\n",
      "     |  [('a', 3), ('c', 1), ('b', 0)]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Counter\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |      Add counts from two counters.\n",
      "     |      \n",
      "     |      >>> Counter('abbb') + Counter('bcc')\n",
      "     |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |      Intersection is the minimum of corresponding counts.\n",
      "     |      \n",
      "     |      >>> Counter('abbb') & Counter('bcc')\n",
      "     |      Counter({'b': 1})\n",
      "     |  \n",
      "     |  __delitem__(self, elem)\n",
      "     |      Like dict.__delitem__() but does not raise KeyError for missing values.\n",
      "     |  \n",
      "     |  __iadd__(self, other)\n",
      "     |      Inplace add from another counter, keeping only positive counts.\n",
      "     |      \n",
      "     |      >>> c = Counter('abbb')\n",
      "     |      >>> c += Counter('bcc')\n",
      "     |      >>> c\n",
      "     |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      "     |  \n",
      "     |  __iand__(self, other)\n",
      "     |      Inplace intersection is the minimum of corresponding counts.\n",
      "     |      \n",
      "     |      >>> c = Counter('abbb')\n",
      "     |      >>> c &= Counter('bcc')\n",
      "     |      >>> c\n",
      "     |      Counter({'b': 1})\n",
      "     |  \n",
      "     |  __init__(*args, **kwds)\n",
      "     |      Create a new, empty Counter object.  And if given, count elements\n",
      "     |      from an input iterable.  Or, initialize the count from another mapping\n",
      "     |      of elements to their counts.\n",
      "     |      \n",
      "     |      >>> c = Counter()                           # a new, empty counter\n",
      "     |      >>> c = Counter('gallahad')                 # a new counter from an iterable\n",
      "     |      >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n",
      "     |      >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n",
      "     |  \n",
      "     |  __ior__(self, other)\n",
      "     |      Inplace union is the maximum of value from either counter.\n",
      "     |      \n",
      "     |      >>> c = Counter('abbb')\n",
      "     |      >>> c |= Counter('bcc')\n",
      "     |      >>> c\n",
      "     |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      "     |  \n",
      "     |  __isub__(self, other)\n",
      "     |      Inplace subtract counter, but keep only results with positive counts.\n",
      "     |      \n",
      "     |      >>> c = Counter('abbbc')\n",
      "     |      >>> c -= Counter('bccd')\n",
      "     |      >>> c\n",
      "     |      Counter({'b': 2, 'a': 1})\n",
      "     |  \n",
      "     |  __missing__(self, key)\n",
      "     |      The count of elements not in the Counter is zero.\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |      Subtracts from an empty counter.  Strips positive and zero counts,\n",
      "     |      and flips the sign on negative counts.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |      Union is the maximum of value in either of the input counters.\n",
      "     |      \n",
      "     |      >>> Counter('abbb') | Counter('bcc')\n",
      "     |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      "     |  \n",
      "     |  __pos__(self)\n",
      "     |      Adds an empty counter, effectively stripping negative and zero counts\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |      Subtract count, but keep only results with positive counts.\n",
      "     |      \n",
      "     |      >>> Counter('abbbc') - Counter('bccd')\n",
      "     |      Counter({'b': 2, 'a': 1})\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Return a shallow copy.\n",
      "     |  \n",
      "     |  elements(self)\n",
      "     |      Iterator over elements repeating each as many times as its count.\n",
      "     |      \n",
      "     |      >>> c = Counter('ABCABC')\n",
      "     |      >>> sorted(c.elements())\n",
      "     |      ['A', 'A', 'B', 'B', 'C', 'C']\n",
      "     |      \n",
      "     |      # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
      "     |      >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
      "     |      >>> product = 1\n",
      "     |      >>> for factor in prime_factors.elements():     # loop over factors\n",
      "     |      ...     product *= factor                       # and multiply them\n",
      "     |      >>> product\n",
      "     |      1836\n",
      "     |      \n",
      "     |      Note, if an element's count has been set to zero or is a negative\n",
      "     |      number, elements() will ignore it.\n",
      "     |  \n",
      "     |  most_common(self, n=None)\n",
      "     |      List the n most common elements and their counts from the most\n",
      "     |      common to the least.  If n is None, then list all element counts.\n",
      "     |      \n",
      "     |      >>> Counter('abcdeabcdabcaba').most_common(3)\n",
      "     |      [('a', 5), ('b', 4), ('c', 3)]\n",
      "     |  \n",
      "     |  subtract(*args, **kwds)\n",
      "     |      Like dict.update() but subtracts counts instead of replacing them.\n",
      "     |      Counts can be reduced below zero.  Both the inputs and outputs are\n",
      "     |      allowed to contain zero and negative counts.\n",
      "     |      \n",
      "     |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      "     |      \n",
      "     |      >>> c = Counter('which')\n",
      "     |      >>> c.subtract('witch')             # subtract elements from another iterable\n",
      "     |      >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
      "     |      >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
      "     |      0\n",
      "     |      >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
      "     |      -1\n",
      "     |  \n",
      "     |  update(*args, **kwds)\n",
      "     |      Like dict.update() but add counts instead of replacing them.\n",
      "     |      \n",
      "     |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      "     |      \n",
      "     |      >>> c = Counter('which')\n",
      "     |      >>> c.update('witch')           # add elements from another iterable\n",
      "     |      >>> d = Counter('watch')\n",
      "     |      >>> c.update(d)                 # add elements from another counter\n",
      "     |      >>> c['h']                      # four 'h' in which, witch, and watch\n",
      "     |      4\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromkeys(iterable, v=None) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(...)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      "     |      2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OrderedDict(builtins.dict)\n",
      "     |  Dictionary that remembers insertion order\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      od.pop(k[,d]) -> v, remove specified key and return the corresponding\n",
      "     |      value.  If key is not found, d is returned if given, otherwise KeyError\n",
      "     |      is raised.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  setdefault(self, /, key, default=None)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class UserDict(collections.abc.MutableMapping)\n",
      "     |  UserDict(*args, **kwargs)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UserDict\n",
      "     |      collections.abc.MutableMapping\n",
      "     |      collections.abc.Mapping\n",
      "     |      collections.abc.Collection\n",
      "     |      collections.abc.Sized\n",
      "     |      collections.abc.Iterable\n",
      "     |      collections.abc.Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, key)\n",
      "     |      # Modify __contains__ to work correctly when __missing__ is present\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __delitem__(self, key)\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |  \n",
      "     |  __init__(*args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setitem__(self, key, item)\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from abc.ABCMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.MutableMapping:\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  pop(self, key, default=<object object at 0x000002630B5D7120>)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised.\n",
      "     |  \n",
      "     |  popitem(self)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair\n",
      "     |      as a 2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None)\n",
      "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      "     |  \n",
      "     |  update(*args, **kwds)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
      "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k, v in F.items(): D[k] = v\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.Mapping:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  get(self, key, default=None)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  items(self)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(self)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  values(self)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from collections.abc.Mapping:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __reversed__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.abc.Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class UserList(collections.abc.MutableSequence)\n",
      "     |  UserList(initlist=None)\n",
      "     |  \n",
      "     |  A more or less complete user-defined wrapper around list objects.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UserList\n",
      "     |      collections.abc.MutableSequence\n",
      "     |      collections.abc.Sequence\n",
      "     |      collections.abc.Reversible\n",
      "     |      collections.abc.Collection\n",
      "     |      collections.abc.Sized\n",
      "     |      collections.abc.Iterable\n",
      "     |      collections.abc.Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __contains__(self, item)\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __delitem__(self, i)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iadd__(self, other)\n",
      "     |  \n",
      "     |  __imul__(self, n)\n",
      "     |  \n",
      "     |  __init__(self, initlist=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, n)\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmul__ = __mul__(self, n)\n",
      "     |  \n",
      "     |  __setitem__(self, i, item)\n",
      "     |  \n",
      "     |  append(self, item)\n",
      "     |      S.append(value) -- append value to the end of the sequence\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      S.clear() -> None -- remove all items from S\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |  \n",
      "     |  count(self, item)\n",
      "     |      S.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  extend(self, other)\n",
      "     |      S.extend(iterable) -- extend sequence by appending elements from the iterable\n",
      "     |  \n",
      "     |  index(self, item, *args)\n",
      "     |      S.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |      \n",
      "     |      Supporting start and stop arguments is optional, but\n",
      "     |      recommended.\n",
      "     |  \n",
      "     |  insert(self, i, item)\n",
      "     |      S.insert(index, value) -- insert value before index\n",
      "     |  \n",
      "     |  pop(self, i=-1)\n",
      "     |      S.pop([index]) -> item -- remove and return item at index (default last).\n",
      "     |      Raise IndexError if list is empty or index is out of range.\n",
      "     |  \n",
      "     |  remove(self, item)\n",
      "     |      S.remove(value) -- remove first occurrence of value.\n",
      "     |      Raise ValueError if the value is not present.\n",
      "     |  \n",
      "     |  reverse(self)\n",
      "     |      S.reverse() -- reverse *IN PLACE*\n",
      "     |  \n",
      "     |  sort(self, *args, **kwds)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.Sequence:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.abc.Reversible:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class UserString(collections.abc.Sequence)\n",
      "     |  UserString(seq)\n",
      "     |  \n",
      "     |  All the operations on a read-only sequence.\n",
      "     |  \n",
      "     |  Concrete subclasses must override __new__ or __init__,\n",
      "     |  __getitem__, and __len__.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UserString\n",
      "     |      collections.abc.Sequence\n",
      "     |      collections.abc.Reversible\n",
      "     |      collections.abc.Collection\n",
      "     |      collections.abc.Sized\n",
      "     |      collections.abc.Iterable\n",
      "     |      collections.abc.Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |  \n",
      "     |  __complex__(self)\n",
      "     |  \n",
      "     |  __contains__(self, char)\n",
      "     |  \n",
      "     |  __eq__(self, string)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __float__(self)\n",
      "     |  \n",
      "     |  __ge__(self, string)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __getnewargs__(self)\n",
      "     |  \n",
      "     |  __gt__(self, string)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __init__(self, seq)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __int__(self)\n",
      "     |  \n",
      "     |  __le__(self, string)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __lt__(self, string)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mod__(self, args)\n",
      "     |  \n",
      "     |  __mul__(self, n)\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmod__(self, format)\n",
      "     |  \n",
      "     |  __rmul__ = __mul__(self, n)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  capitalize(self)\n",
      "     |      # the following methods are defined in alphabetical order:\n",
      "     |  \n",
      "     |  casefold(self)\n",
      "     |  \n",
      "     |  center(self, width, *args)\n",
      "     |  \n",
      "     |  count(self, sub, start=0, end=9223372036854775807)\n",
      "     |      S.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  encode(self, encoding=None, errors=None)\n",
      "     |  \n",
      "     |  endswith(self, suffix, start=0, end=9223372036854775807)\n",
      "     |  \n",
      "     |  expandtabs(self, tabsize=8)\n",
      "     |  \n",
      "     |  find(self, sub, start=0, end=9223372036854775807)\n",
      "     |  \n",
      "     |  format(self, *args, **kwds)\n",
      "     |  \n",
      "     |  format_map(self, mapping)\n",
      "     |  \n",
      "     |  index(self, sub, start=0, end=9223372036854775807)\n",
      "     |      S.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |      \n",
      "     |      Supporting start and stop arguments is optional, but\n",
      "     |      recommended.\n",
      "     |  \n",
      "     |  isalnum(self)\n",
      "     |  \n",
      "     |  isalpha(self)\n",
      "     |  \n",
      "     |  isascii(self)\n",
      "     |  \n",
      "     |  isdecimal(self)\n",
      "     |  \n",
      "     |  isdigit(self)\n",
      "     |  \n",
      "     |  isidentifier(self)\n",
      "     |  \n",
      "     |  islower(self)\n",
      "     |  \n",
      "     |  isnumeric(self)\n",
      "     |  \n",
      "     |  isprintable(self)\n",
      "     |  \n",
      "     |  isspace(self)\n",
      "     |  \n",
      "     |  istitle(self)\n",
      "     |  \n",
      "     |  isupper(self)\n",
      "     |  \n",
      "     |  join(self, seq)\n",
      "     |  \n",
      "     |  ljust(self, width, *args)\n",
      "     |  \n",
      "     |  lower(self)\n",
      "     |  \n",
      "     |  lstrip(self, chars=None)\n",
      "     |  \n",
      "     |  partition(self, sep)\n",
      "     |  \n",
      "     |  replace(self, old, new, maxsplit=-1)\n",
      "     |  \n",
      "     |  rfind(self, sub, start=0, end=9223372036854775807)\n",
      "     |  \n",
      "     |  rindex(self, sub, start=0, end=9223372036854775807)\n",
      "     |  \n",
      "     |  rjust(self, width, *args)\n",
      "     |  \n",
      "     |  rpartition(self, sep)\n",
      "     |  \n",
      "     |  rsplit(self, sep=None, maxsplit=-1)\n",
      "     |  \n",
      "     |  rstrip(self, chars=None)\n",
      "     |  \n",
      "     |  split(self, sep=None, maxsplit=-1)\n",
      "     |  \n",
      "     |  splitlines(self, keepends=False)\n",
      "     |  \n",
      "     |  startswith(self, prefix, start=0, end=9223372036854775807)\n",
      "     |  \n",
      "     |  strip(self, chars=None)\n",
      "     |  \n",
      "     |  swapcase(self)\n",
      "     |  \n",
      "     |  title(self)\n",
      "     |  \n",
      "     |  translate(self, *args)\n",
      "     |  \n",
      "     |  upper(self)\n",
      "     |  \n",
      "     |  zfill(self, width)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  maketrans(x, y=None, z=None, /)\n",
      "     |      Return a translation table usable for str.translate().\n",
      "     |      \n",
      "     |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      "     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      "     |      Character keys will be then converted to ordinals.\n",
      "     |      If there are two arguments, they must be strings of equal length, and\n",
      "     |      in the resulting dictionary, each character in x will be mapped to the\n",
      "     |      character at the same position in y. If there is a third argument, it\n",
      "     |      must be a string, whose characters will be mapped to None in the result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.abc.Sequence:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.abc.Reversible:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class defaultdict(builtins.dict)\n",
      "     |  defaultdict(default_factory[, ...]) --> dict with default factory\n",
      "     |  \n",
      "     |  The default factory is called without arguments to produce\n",
      "     |  a new value when a key is not present, in __getitem__ only.\n",
      "     |  A defaultdict compares equal to a dict with the same items.\n",
      "     |  All remaining arguments are treated the same as if they were\n",
      "     |  passed to the dict constructor, including keyword arguments.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      defaultdict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(...)\n",
      "     |      D.copy() -> a shallow copy of D.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __missing__(...)\n",
      "     |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
      "     |      if self.default_factory is None: raise KeyError((key,))\n",
      "     |      self[key] = value = self.default_factory()\n",
      "     |      return value\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Return state information for pickling.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  default_factory\n",
      "     |      Factory for default value called by __missing__().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(...)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      "     |      2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class deque(builtins.object)\n",
      "     |  deque([iterable[, maxlen]]) --> deque object\n",
      "     |  \n",
      "     |  A list-like sequence optimized for data accesses near its endpoints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __bool__(self, /)\n",
      "     |      self != 0\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __copy__(...)\n",
      "     |      Return a shallow copy of a deque.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iadd__(self, value, /)\n",
      "     |      Implement self+=value.\n",
      "     |  \n",
      "     |  __imul__(self, value, /)\n",
      "     |      Implement self*=value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Return state information for pickling.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      D.__reversed__() -- return a reverse iterator over the deque\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -- size of D in memory, in bytes\n",
      "     |  \n",
      "     |  append(...)\n",
      "     |      Add an element to the right side of the deque.\n",
      "     |  \n",
      "     |  appendleft(...)\n",
      "     |      Add an element to the left side of the deque.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      Remove all elements from the deque.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      Return a shallow copy of a deque.\n",
      "     |  \n",
      "     |  count(...)\n",
      "     |      D.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  extend(...)\n",
      "     |      Extend the right side of the deque with elements from the iterable\n",
      "     |  \n",
      "     |  extendleft(...)\n",
      "     |      Extend the left side of the deque with elements from the iterable\n",
      "     |  \n",
      "     |  index(...)\n",
      "     |      D.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  insert(...)\n",
      "     |      D.insert(index, object) -- insert object before index\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      Remove and return the rightmost element.\n",
      "     |  \n",
      "     |  popleft(...)\n",
      "     |      Remove and return the leftmost element.\n",
      "     |  \n",
      "     |  remove(...)\n",
      "     |      D.remove(value) -- remove first occurrence of value.\n",
      "     |  \n",
      "     |  reverse(...)\n",
      "     |      D.reverse() -- reverse *IN PLACE*\n",
      "     |  \n",
      "     |  rotate(...)\n",
      "     |      Rotate the deque n steps to the right (default n=1).  If n is negative, rotates left.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  maxlen\n",
      "     |      maximum size of a deque or None if unbounded\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "\n",
      "FUNCTIONS\n",
      "    __getattr__(name)\n",
      "    \n",
      "    namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)\n",
      "        Returns a new subclass of tuple with named fields.\n",
      "        \n",
      "        >>> Point = namedtuple('Point', ['x', 'y'])\n",
      "        >>> Point.__doc__                   # docstring for the new class\n",
      "        'Point(x, y)'\n",
      "        >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n",
      "        >>> p[0] + p[1]                     # indexable like a plain tuple\n",
      "        33\n",
      "        >>> x, y = p                        # unpack like a regular tuple\n",
      "        >>> x, y\n",
      "        (11, 22)\n",
      "        >>> p.x + p.y                       # fields also accessible by name\n",
      "        33\n",
      "        >>> d = p._asdict()                 # convert to a dictionary\n",
      "        >>> d['x']\n",
      "        11\n",
      "        >>> Point(**d)                      # convert from a dictionary\n",
      "        Point(x=11, y=22)\n",
      "        >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n",
      "        Point(x=100, y=22)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['deque', 'defaultdict', 'namedtuple', 'UserDict', 'UserList...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rahul\\anaconda3\\lib\\collections\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just type help\n",
    "help()\n",
    "\n",
    "# then inside help>[TypeModule, class, functions names etc.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```help(object)```\n",
    "objects: ```Package```, ```module```, ```class```, ```function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, safe=True)\n",
      "        Constructs a new estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It yields a new estimator\n",
      "        with the same parameters that has not been fit on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object, or list, tuple or set of objects\n",
      "            The estimator or group of estimators to be cloned\n",
      "        \n",
      "        safe : boolean, optional\n",
      "            If safe is false, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "    \n",
      "    config_context(**new_config)\n",
      "        Context manager for global scikit-learn configuration\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "        print_changed_only : bool, optional\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited. This is not\n",
      "        thread-safe.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN, ...\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config: Set global scikit-learn configuration\n",
      "        get_config: Retrieve current values of the global configuration\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context: Context manager for global scikit-learn configuration\n",
      "        set_config: Set global scikit-learn configuration\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, optional\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, optional\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, optional\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context: Context manager for global scikit-learn configuration\n",
      "        get_config: Retrieve current values of the global configuration\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    0.22.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rahul\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.feature_extraction.text in sklearn.feature_extraction:\n",
      "\n",
      "NAME\n",
      "    sklearn.feature_extraction.text\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to\n",
      "    build feature vectors from text documents.\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "            TfidfVectorizer\n",
      "        HashingVectorizer(sklearn.base.TransformerMixin, _VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "        TfidfTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        HashingVectorizer(sklearn.base.TransformerMixin, _VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "        TfidfTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "    _VectorizerMixin(builtins.object)\n",
      "        CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "            TfidfVectorizer\n",
      "        HashingVectorizer(sklearn.base.TransformerMixin, _VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "    \n",
      "    class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "     |  CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      "     |  \n",
      "     |  Convert a collection of text documents to a matrix of token counts\n",
      "     |  \n",
      "     |  This implementation produces a sparse representation of the counts using\n",
      "     |  scipy.sparse.csr_matrix.\n",
      "     |  \n",
      "     |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      "     |  that does some kind of feature selection then the number of features will\n",
      "     |  be equal to the vocabulary size found by analyzing the data.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  input : string {'filename', 'file', 'content'}\n",
      "     |      If 'filename', the sequence passed as an argument to fit is\n",
      "     |      expected to be a list of filenames that need reading to fetch\n",
      "     |      the raw content to analyze.\n",
      "     |  \n",
      "     |      If 'file', the sequence items must have a 'read' method (file-like\n",
      "     |      object) that is called to fetch the bytes in memory.\n",
      "     |  \n",
      "     |      Otherwise the input is expected to be a sequence of items that\n",
      "     |      can be of type string or byte.\n",
      "     |  \n",
      "     |  encoding : string, 'utf-8' by default.\n",
      "     |      If bytes or files are given to analyze, this encoding is used to\n",
      "     |      decode.\n",
      "     |  \n",
      "     |  decode_error : {'strict', 'ignore', 'replace'}\n",
      "     |      Instruction on what to do if a byte sequence is given to analyze that\n",
      "     |      contains characters not of the given `encoding`. By default, it is\n",
      "     |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "     |      values are 'ignore' and 'replace'.\n",
      "     |  \n",
      "     |  strip_accents : {'ascii', 'unicode', None}\n",
      "     |      Remove accents and perform other character normalization\n",
      "     |      during the preprocessing step.\n",
      "     |      'ascii' is a fast method that only works on characters that have\n",
      "     |      an direct ASCII mapping.\n",
      "     |      'unicode' is a slightly slower method that works on any characters.\n",
      "     |      None (default) does nothing.\n",
      "     |  \n",
      "     |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "     |      :func:`unicodedata.normalize`.\n",
      "     |  \n",
      "     |  lowercase : boolean, True by default\n",
      "     |      Convert all characters to lowercase before tokenizing.\n",
      "     |  \n",
      "     |  preprocessor : callable or None (default)\n",
      "     |      Override the preprocessing (string transformation) stage while\n",
      "     |      preserving the tokenizing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  tokenizer : callable or None (default)\n",
      "     |      Override the string tokenization step while preserving the\n",
      "     |      preprocessing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |  stop_words : string {'english'}, list, or None (default)\n",
      "     |      If 'english', a built-in stop word list for English is used.\n",
      "     |      There are several known issues with 'english' and you should\n",
      "     |      consider an alternative (see :ref:`stop_words`).\n",
      "     |  \n",
      "     |      If a list, that list is assumed to contain stop words, all of which\n",
      "     |      will be removed from the resulting tokens.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |      If None, no stop words will be used. max_df can be set to a value\n",
      "     |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "     |      words based on intra corpus document frequency of terms.\n",
      "     |  \n",
      "     |  token_pattern : string\n",
      "     |      Regular expression denoting what constitutes a \"token\", only used\n",
      "     |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      "     |      or more alphanumeric characters (punctuation is completely ignored\n",
      "     |      and always treated as a token separator).\n",
      "     |  \n",
      "     |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "     |      The lower and upper boundary of the range of n-values for different\n",
      "     |      word n-grams or char n-grams to be extracted. All values of n such\n",
      "     |      such that min_n <= n <= max_n will be used. For example an\n",
      "     |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      "     |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      "     |      Whether the feature should be made of word n-gram or character\n",
      "     |      n-grams.\n",
      "     |      Option 'char_wb' creates character n-grams only from text inside\n",
      "     |      word boundaries; n-grams at the edges of words are padded with space.\n",
      "     |  \n",
      "     |      If a callable is passed it is used to extract the sequence of features\n",
      "     |      out of the raw, unprocessed input.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |  \n",
      "     |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "     |      first read from the file and then passed to the given callable\n",
      "     |      analyzer.\n",
      "     |  \n",
      "     |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      "     |      When building the vocabulary ignore terms that have a document\n",
      "     |      frequency strictly higher than the given threshold (corpus-specific\n",
      "     |      stop words).\n",
      "     |      If float, the parameter represents a proportion of documents, integer\n",
      "     |      absolute counts.\n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      "     |      When building the vocabulary ignore terms that have a document\n",
      "     |      frequency strictly lower than the given threshold. This value is also\n",
      "     |      called cut-off in the literature.\n",
      "     |      If float, the parameter represents a proportion of documents, integer\n",
      "     |      absolute counts.\n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  max_features : int or None, default=None\n",
      "     |      If not None, build a vocabulary that only consider the top\n",
      "     |      max_features ordered by term frequency across the corpus.\n",
      "     |  \n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  vocabulary : Mapping or iterable, optional\n",
      "     |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "     |      indices in the feature matrix, or an iterable over terms. If not\n",
      "     |      given, a vocabulary is determined from the input documents. Indices\n",
      "     |      in the mapping should not be repeated and should not have any gap\n",
      "     |      between 0 and the largest index.\n",
      "     |  \n",
      "     |  binary : boolean, default=False\n",
      "     |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      "     |      probabilistic models that model binary events rather than integer\n",
      "     |      counts.\n",
      "     |  \n",
      "     |  dtype : type, optional\n",
      "     |      Type of the matrix returned by fit_transform() or transform().\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  vocabulary_ : dict\n",
      "     |      A mapping of terms to feature indices.\n",
      "     |  \n",
      "     |  fixed_vocabulary_: boolean\n",
      "     |      True if a fixed vocabulary of term to indices mapping\n",
      "     |      is provided by the user\n",
      "     |  \n",
      "     |  stop_words_ : set\n",
      "     |      Terms that were ignored because they either:\n",
      "     |  \n",
      "     |        - occurred in too many documents (`max_df`)\n",
      "     |        - occurred in too few documents (`min_df`)\n",
      "     |        - were cut off by feature selection (`max_features`).\n",
      "     |  \n",
      "     |      This is only available if no vocabulary was given.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      "     |  >>> corpus = [\n",
      "     |  ...     'This is the first document.',\n",
      "     |  ...     'This document is the second document.',\n",
      "     |  ...     'And this is the third one.',\n",
      "     |  ...     'Is this the first document?',\n",
      "     |  ... ]\n",
      "     |  >>> vectorizer = CountVectorizer()\n",
      "     |  >>> X = vectorizer.fit_transform(corpus)\n",
      "     |  >>> print(vectorizer.get_feature_names())\n",
      "     |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "     |  >>> print(X.toarray())\n",
      "     |  [[0 1 1 1 0 0 1 0 1]\n",
      "     |   [0 2 0 1 0 1 1 0 1]\n",
      "     |   [1 0 0 1 1 0 1 1 1]\n",
      "     |   [0 1 1 1 0 0 1 0 1]]\n",
      "     |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      "     |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      "     |  >>> print(vectorizer2.get_feature_names())\n",
      "     |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      "     |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      "     |   'this document', 'this is', 'this the']\n",
      "     |   >>> print(X2.toarray())\n",
      "     |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      "     |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      "     |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      "     |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HashingVectorizer, TfidfVectorizer\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The ``stop_words_`` attribute can get large and increase the model size\n",
      "     |  when pickling. This attribute is provided only for introspection and can\n",
      "     |  be safely removed using delattr or set to None before pickling.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CountVectorizer\n",
      "     |      _VectorizerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, raw_documents, y=None)\n",
      "     |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  fit_transform(self, raw_documents, y=None)\n",
      "     |      Learn the vocabulary dictionary and return term-document matrix.\n",
      "     |      \n",
      "     |      This is equivalent to fit followed by transform, but more efficiently\n",
      "     |      implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, [n_samples, n_features]\n",
      "     |          Document-term matrix.\n",
      "     |  \n",
      "     |  get_feature_names(self)\n",
      "     |      Array mapping from feature integer indices to feature name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list\n",
      "     |          A list of feature names.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Return terms per document with nonzero entries in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Document-term matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_inv : list of arrays, len = n_samples\n",
      "     |          List of arrays of terms.\n",
      "     |  \n",
      "     |  transform(self, raw_documents)\n",
      "     |      Transform documents to document-term matrix.\n",
      "     |      \n",
      "     |      Extract token counts out of raw text documents using the vocabulary\n",
      "     |      fitted with fit or the one provided to the constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : sparse matrix, [n_samples, n_features]\n",
      "     |          Document-term matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _VectorizerMixin:\n",
      "     |  \n",
      "     |  build_analyzer(self)\n",
      "     |      Return a callable that handles preprocessing, tokenization\n",
      "     |      and n-grams generation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      analyzer: callable\n",
      "     |          A function to handle preprocessing, tokenization\n",
      "     |          and n-grams generation.\n",
      "     |  \n",
      "     |  build_preprocessor(self)\n",
      "     |      Return a function to preprocess the text before tokenization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      preprocessor: callable\n",
      "     |            A function to preprocess the text before tokenization.\n",
      "     |  \n",
      "     |  build_tokenizer(self)\n",
      "     |      Return a function that splits a string into a sequence of tokens.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tokenizer: callable\n",
      "     |            A function to split a string into a sequence of tokens.\n",
      "     |  \n",
      "     |  decode(self, doc)\n",
      "     |      Decode the input into a string of unicode symbols.\n",
      "     |      \n",
      "     |      The decoding strategy depends on the vectorizer parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      doc : str\n",
      "     |          The string to decode.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      doc: str\n",
      "     |          A string of unicode symbols.\n",
      "     |  \n",
      "     |  get_stop_words(self)\n",
      "     |      Build or fetch the effective stop words list.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stop_words: list or None\n",
      "     |              A list of stop words.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _VectorizerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class HashingVectorizer(sklearn.base.TransformerMixin, _VectorizerMixin, sklearn.base.BaseEstimator)\n",
      "     |  HashingVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', n_features=1048576, binary=False, norm='l2', alternate_sign=True, dtype=<class 'numpy.float64'>)\n",
      "     |  \n",
      "     |  Convert a collection of text documents to a matrix of token occurrences\n",
      "     |  \n",
      "     |  It turns a collection of text documents into a scipy.sparse matrix holding\n",
      "     |  token occurrence counts (or binary occurrence information), possibly\n",
      "     |  normalized as token frequencies if norm='l1' or projected on the euclidean\n",
      "     |  unit sphere if norm='l2'.\n",
      "     |  \n",
      "     |  This text vectorizer implementation uses the hashing trick to find the\n",
      "     |  token string name to feature integer index mapping.\n",
      "     |  \n",
      "     |  This strategy has several advantages:\n",
      "     |  \n",
      "     |  - it is very low memory scalable to large datasets as there is no need to\n",
      "     |    store a vocabulary dictionary in memory\n",
      "     |  \n",
      "     |  - it is fast to pickle and un-pickle as it holds no state besides the\n",
      "     |    constructor parameters\n",
      "     |  \n",
      "     |  - it can be used in a streaming (partial fit) or parallel pipeline as there\n",
      "     |    is no state computed during fit.\n",
      "     |  \n",
      "     |  There are also a couple of cons (vs using a CountVectorizer with an\n",
      "     |  in-memory vocabulary):\n",
      "     |  \n",
      "     |  - there is no way to compute the inverse transform (from feature indices to\n",
      "     |    string feature names) which can be a problem when trying to introspect\n",
      "     |    which features are most important to a model.\n",
      "     |  \n",
      "     |  - there can be collisions: distinct tokens can be mapped to the same\n",
      "     |    feature index. However in practice this is rarely an issue if n_features\n",
      "     |    is large enough (e.g. 2 ** 18 for text classification problems).\n",
      "     |  \n",
      "     |  - no IDF weighting as this would render the transformer stateful.\n",
      "     |  \n",
      "     |  The hash function employed is the signed 32-bit version of Murmurhash3.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  input : string {'filename', 'file', 'content'}\n",
      "     |      If 'filename', the sequence passed as an argument to fit is\n",
      "     |      expected to be a list of filenames that need reading to fetch\n",
      "     |      the raw content to analyze.\n",
      "     |  \n",
      "     |      If 'file', the sequence items must have a 'read' method (file-like\n",
      "     |      object) that is called to fetch the bytes in memory.\n",
      "     |  \n",
      "     |      Otherwise the input is expected to be a sequence of items that\n",
      "     |      can be of type string or byte.\n",
      "     |  \n",
      "     |  encoding : string, default='utf-8'\n",
      "     |      If bytes or files are given to analyze, this encoding is used to\n",
      "     |      decode.\n",
      "     |  \n",
      "     |  decode_error : {'strict', 'ignore', 'replace'}\n",
      "     |      Instruction on what to do if a byte sequence is given to analyze that\n",
      "     |      contains characters not of the given `encoding`. By default, it is\n",
      "     |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "     |      values are 'ignore' and 'replace'.\n",
      "     |  \n",
      "     |  strip_accents : {'ascii', 'unicode', None}\n",
      "     |      Remove accents and perform other character normalization\n",
      "     |      during the preprocessing step.\n",
      "     |      'ascii' is a fast method that only works on characters that have\n",
      "     |      an direct ASCII mapping.\n",
      "     |      'unicode' is a slightly slower method that works on any characters.\n",
      "     |      None (default) does nothing.\n",
      "     |  \n",
      "     |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "     |      :func:`unicodedata.normalize`.\n",
      "     |  \n",
      "     |  lowercase : boolean, default=True\n",
      "     |      Convert all characters to lowercase before tokenizing.\n",
      "     |  \n",
      "     |  preprocessor : callable or None (default)\n",
      "     |      Override the preprocessing (string transformation) stage while\n",
      "     |      preserving the tokenizing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  tokenizer : callable or None (default)\n",
      "     |      Override the string tokenization step while preserving the\n",
      "     |      preprocessing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |  stop_words : string {'english'}, list, or None (default)\n",
      "     |      If 'english', a built-in stop word list for English is used.\n",
      "     |      There are several known issues with 'english' and you should\n",
      "     |      consider an alternative (see :ref:`stop_words`).\n",
      "     |  \n",
      "     |      If a list, that list is assumed to contain stop words, all of which\n",
      "     |      will be removed from the resulting tokens.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |  token_pattern : string\n",
      "     |      Regular expression denoting what constitutes a \"token\", only used\n",
      "     |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "     |      or more alphanumeric characters (punctuation is completely ignored\n",
      "     |      and always treated as a token separator).\n",
      "     |  \n",
      "     |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "     |      The lower and upper boundary of the range of n-values for different\n",
      "     |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "     |      will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "     |      unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "     |      only bigrams.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      "     |      Whether the feature should be made of word or character n-grams.\n",
      "     |      Option 'char_wb' creates character n-grams only from text inside\n",
      "     |      word boundaries; n-grams at the edges of words are padded with space.\n",
      "     |  \n",
      "     |      If a callable is passed it is used to extract the sequence of features\n",
      "     |      out of the raw, unprocessed input.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |  \n",
      "     |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "     |      first read from the file and then passed to the given callable\n",
      "     |      analyzer.\n",
      "     |  \n",
      "     |  n_features : integer, default=(2 ** 20)\n",
      "     |      The number of features (columns) in the output matrices. Small numbers\n",
      "     |      of features are likely to cause hash collisions, but large numbers\n",
      "     |      will cause larger coefficient dimensions in linear learners.\n",
      "     |  \n",
      "     |  binary : boolean, default=False.\n",
      "     |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      "     |      probabilistic models that model binary events rather than integer\n",
      "     |      counts.\n",
      "     |  \n",
      "     |  norm : 'l1', 'l2' or None, optional\n",
      "     |      Norm used to normalize term vectors. None for no normalization.\n",
      "     |  \n",
      "     |  alternate_sign : boolean, optional, default True\n",
      "     |      When True, an alternating sign is added to the features as to\n",
      "     |      approximately conserve the inner product in the hashed space even for\n",
      "     |      small n_features. This approach is similar to sparse random projection.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  dtype : type, optional\n",
      "     |      Type of the matrix returned by fit_transform() or transform().\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_extraction.text import HashingVectorizer\n",
      "     |  >>> corpus = [\n",
      "     |  ...     'This is the first document.',\n",
      "     |  ...     'This document is the second document.',\n",
      "     |  ...     'And this is the third one.',\n",
      "     |  ...     'Is this the first document?',\n",
      "     |  ... ]\n",
      "     |  >>> vectorizer = HashingVectorizer(n_features=2**4)\n",
      "     |  >>> X = vectorizer.fit_transform(corpus)\n",
      "     |  >>> print(X.shape)\n",
      "     |  (4, 16)\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  CountVectorizer, TfidfVectorizer\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HashingVectorizer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      _VectorizerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', n_features=1048576, binary=False, norm='l2', alternate_sign=True, dtype=<class 'numpy.float64'>)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Does nothing: this transformer is stateless.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None)\n",
      "     |      Transform a sequence of documents to a document-term matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable over raw text documents, length = n_samples\n",
      "     |          Samples. Each sample must be a text document (either bytes or\n",
      "     |          unicode strings, file name or file object depending on the\n",
      "     |          constructor argument) which will be tokenized and hashed.\n",
      "     |      y : any\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          sklearn.pipeline.Pipeline.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : sparse matrix of shape (n_samples, n_features)\n",
      "     |          Document-term matrix.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None)\n",
      "     |      Does nothing: this transformer is stateless.\n",
      "     |      \n",
      "     |      This method is just there to mark the fact that this transformer\n",
      "     |      can work in a streaming setup.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape [n_samples, n_features]\n",
      "     |          Training data.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform a sequence of documents to a document-term matrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable over raw text documents, length = n_samples\n",
      "     |          Samples. Each sample must be a text document (either bytes or\n",
      "     |          unicode strings, file name or file object depending on the\n",
      "     |          constructor argument) which will be tokenized and hashed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : sparse matrix of shape (n_samples, n_features)\n",
      "     |          Document-term matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _VectorizerMixin:\n",
      "     |  \n",
      "     |  build_analyzer(self)\n",
      "     |      Return a callable that handles preprocessing, tokenization\n",
      "     |      and n-grams generation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      analyzer: callable\n",
      "     |          A function to handle preprocessing, tokenization\n",
      "     |          and n-grams generation.\n",
      "     |  \n",
      "     |  build_preprocessor(self)\n",
      "     |      Return a function to preprocess the text before tokenization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      preprocessor: callable\n",
      "     |            A function to preprocess the text before tokenization.\n",
      "     |  \n",
      "     |  build_tokenizer(self)\n",
      "     |      Return a function that splits a string into a sequence of tokens.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tokenizer: callable\n",
      "     |            A function to split a string into a sequence of tokens.\n",
      "     |  \n",
      "     |  decode(self, doc)\n",
      "     |      Decode the input into a string of unicode symbols.\n",
      "     |      \n",
      "     |      The decoding strategy depends on the vectorizer parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      doc : str\n",
      "     |          The string to decode.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      doc: str\n",
      "     |          A string of unicode symbols.\n",
      "     |  \n",
      "     |  get_stop_words(self)\n",
      "     |      Build or fetch the effective stop words list.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stop_words: list or None\n",
      "     |              A list of stop words.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class TfidfTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      "     |  TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      "     |  \n",
      "     |  Transform a count matrix to a normalized tf or tf-idf representation\n",
      "     |  \n",
      "     |  Tf means term-frequency while tf-idf means term-frequency times inverse\n",
      "     |  document-frequency. This is a common term weighting scheme in information\n",
      "     |  retrieval, that has also found good use in document classification.\n",
      "     |  \n",
      "     |  The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
      "     |  token in a given document is to scale down the impact of tokens that occur\n",
      "     |  very frequently in a given corpus and that are hence empirically less\n",
      "     |  informative than features that occur in a small fraction of the training\n",
      "     |  corpus.\n",
      "     |  \n",
      "     |  The formula that is used to compute the tf-idf for a term t of a document d\n",
      "     |  in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
      "     |  computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
      "     |  n is the total number of documents in the document set and df(t) is the\n",
      "     |  document frequency of t; the document frequency is the number of documents\n",
      "     |  in the document set that contain the term t. The effect of adding \"1\" to\n",
      "     |  the idf in the equation above is that terms with zero idf, i.e., terms\n",
      "     |  that occur in all documents in a training set, will not be entirely\n",
      "     |  ignored.\n",
      "     |  (Note that the idf formula above differs from the standard textbook\n",
      "     |  notation that defines the idf as\n",
      "     |  idf(t) = log [ n / (df(t) + 1) ]).\n",
      "     |  \n",
      "     |  If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
      "     |  numerator and denominator of the idf as if an extra document was seen\n",
      "     |  containing every term in the collection exactly once, which prevents\n",
      "     |  zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n",
      "     |  \n",
      "     |  Furthermore, the formulas used to compute tf and idf depend\n",
      "     |  on parameter settings that correspond to the SMART notation used in IR\n",
      "     |  as follows:\n",
      "     |  \n",
      "     |  Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
      "     |  ``sublinear_tf=True``.\n",
      "     |  Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
      "     |  Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
      "     |  when ``norm=None``.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  norm : 'l1', 'l2' or None, optional (default='l2')\n",
      "     |      Each output row will have unit norm, either:\n",
      "     |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "     |      similarity between two vectors is their dot product when l2 norm has\n",
      "     |      been applied.\n",
      "     |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      "     |      See :func:`preprocessing.normalize`\n",
      "     |  \n",
      "     |  use_idf : boolean (default=True)\n",
      "     |      Enable inverse-document-frequency reweighting.\n",
      "     |  \n",
      "     |  smooth_idf : boolean (default=True)\n",
      "     |      Smooth idf weights by adding one to document frequencies, as if an\n",
      "     |      extra document was seen containing every term in the collection\n",
      "     |      exactly once. Prevents zero divisions.\n",
      "     |  \n",
      "     |  sublinear_tf : boolean (default=False)\n",
      "     |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  idf_ : array, shape (n_features)\n",
      "     |      The inverse document frequency (IDF) vector; only defined\n",
      "     |      if  ``use_idf`` is True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_extraction.text import TfidfTransformer\n",
      "     |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      "     |  >>> from sklearn.pipeline import Pipeline\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> corpus = ['this is the first document',\n",
      "     |  ...           'this document is the second document',\n",
      "     |  ...           'and this is the third one',\n",
      "     |  ...           'is this the first document']\n",
      "     |  >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
      "     |  ...               'and', 'one']\n",
      "     |  >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
      "     |  ...                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
      "     |  >>> pipe['count'].transform(corpus).toarray()\n",
      "     |  array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
      "     |         [1, 2, 0, 1, 1, 1, 0, 0],\n",
      "     |         [1, 0, 0, 1, 0, 1, 1, 1],\n",
      "     |         [1, 1, 1, 1, 0, 1, 0, 0]])\n",
      "     |  >>> pipe['tfid'].idf_\n",
      "     |  array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
      "     |         1.        , 1.91629073, 1.91629073])\n",
      "     |  >>> pipe.transform(corpus).shape\n",
      "     |  (4, 8)\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
      "     |                 Information Retrieval. Addison Wesley, pp. 68-74.\n",
      "     |  \n",
      "     |  .. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).\n",
      "     |                 Introduction to Information Retrieval. Cambridge University\n",
      "     |                 Press, pp. 118-120.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TfidfTransformer\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Learn the idf vector (global term weights)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : sparse matrix, [n_samples, n_features]\n",
      "     |          a matrix of term/token counts\n",
      "     |  \n",
      "     |  transform(self, X, copy=True)\n",
      "     |      Transform a count matrix to a tf or tf-idf representation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : sparse matrix, [n_samples, n_features]\n",
      "     |          a matrix of term/token counts\n",
      "     |      \n",
      "     |      copy : boolean, default True\n",
      "     |          Whether to copy X and operate on the copy or perform in-place\n",
      "     |          operations.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      vectors : sparse matrix, [n_samples, n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  idf_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class TfidfVectorizer(CountVectorizer)\n",
      "     |  TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      "     |  \n",
      "     |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "     |  \n",
      "     |  Equivalent to :class:`CountVectorizer` followed by\n",
      "     |  :class:`TfidfTransformer`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  input : str {'filename', 'file', 'content'}\n",
      "     |      If 'filename', the sequence passed as an argument to fit is\n",
      "     |      expected to be a list of filenames that need reading to fetch\n",
      "     |      the raw content to analyze.\n",
      "     |  \n",
      "     |      If 'file', the sequence items must have a 'read' method (file-like\n",
      "     |      object) that is called to fetch the bytes in memory.\n",
      "     |  \n",
      "     |      Otherwise the input is expected to be a sequence of items that\n",
      "     |      can be of type string or byte.\n",
      "     |  \n",
      "     |  encoding : str, default='utf-8'\n",
      "     |      If bytes or files are given to analyze, this encoding is used to\n",
      "     |      decode.\n",
      "     |  \n",
      "     |  decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
      "     |      Instruction on what to do if a byte sequence is given to analyze that\n",
      "     |      contains characters not of the given `encoding`. By default, it is\n",
      "     |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "     |      values are 'ignore' and 'replace'.\n",
      "     |  \n",
      "     |  strip_accents : {'ascii', 'unicode', None} (default=None)\n",
      "     |      Remove accents and perform other character normalization\n",
      "     |      during the preprocessing step.\n",
      "     |      'ascii' is a fast method that only works on characters that have\n",
      "     |      an direct ASCII mapping.\n",
      "     |      'unicode' is a slightly slower method that works on any characters.\n",
      "     |      None (default) does nothing.\n",
      "     |  \n",
      "     |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "     |      :func:`unicodedata.normalize`.\n",
      "     |  \n",
      "     |  lowercase : bool (default=True)\n",
      "     |      Convert all characters to lowercase before tokenizing.\n",
      "     |  \n",
      "     |  preprocessor : callable or None (default=None)\n",
      "     |      Override the preprocessing (string transformation) stage while\n",
      "     |      preserving the tokenizing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  tokenizer : callable or None (default=None)\n",
      "     |      Override the string tokenization step while preserving the\n",
      "     |      preprocessing and n-grams generation steps.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |  analyzer : str, {'word', 'char', 'char_wb'} or callable\n",
      "     |      Whether the feature should be made of word or character n-grams.\n",
      "     |      Option 'char_wb' creates character n-grams only from text inside\n",
      "     |      word boundaries; n-grams at the edges of words are padded with space.\n",
      "     |  \n",
      "     |      If a callable is passed it is used to extract the sequence of features\n",
      "     |      out of the raw, unprocessed input.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |  \n",
      "     |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      "     |      first read from the file and then passed to the given callable\n",
      "     |      analyzer.\n",
      "     |  \n",
      "     |  stop_words : str {'english'}, list, or None (default=None)\n",
      "     |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "     |      list is returned. 'english' is currently the only supported string\n",
      "     |      value.\n",
      "     |      There are several known issues with 'english' and you should\n",
      "     |      consider an alternative (see :ref:`stop_words`).\n",
      "     |  \n",
      "     |      If a list, that list is assumed to contain stop words, all of which\n",
      "     |      will be removed from the resulting tokens.\n",
      "     |      Only applies if ``analyzer == 'word'``.\n",
      "     |  \n",
      "     |      If None, no stop words will be used. max_df can be set to a value\n",
      "     |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      "     |      words based on intra corpus document frequency of terms.\n",
      "     |  \n",
      "     |  token_pattern : str\n",
      "     |      Regular expression denoting what constitutes a \"token\", only used\n",
      "     |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "     |      or more alphanumeric characters (punctuation is completely ignored\n",
      "     |      and always treated as a token separator).\n",
      "     |  \n",
      "     |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "     |      The lower and upper boundary of the range of n-values for different\n",
      "     |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "     |      will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "     |      unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "     |      only bigrams.\n",
      "     |      Only applies if ``analyzer is not callable``.\n",
      "     |  \n",
      "     |  max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
      "     |      When building the vocabulary ignore terms that have a document\n",
      "     |      frequency strictly higher than the given threshold (corpus-specific\n",
      "     |      stop words).\n",
      "     |      If float, the parameter represents a proportion of documents, integer\n",
      "     |      absolute counts.\n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  min_df : float in range [0.0, 1.0] or int (default=1)\n",
      "     |      When building the vocabulary ignore terms that have a document\n",
      "     |      frequency strictly lower than the given threshold. This value is also\n",
      "     |      called cut-off in the literature.\n",
      "     |      If float, the parameter represents a proportion of documents, integer\n",
      "     |      absolute counts.\n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  max_features : int or None (default=None)\n",
      "     |      If not None, build a vocabulary that only consider the top\n",
      "     |      max_features ordered by term frequency across the corpus.\n",
      "     |  \n",
      "     |      This parameter is ignored if vocabulary is not None.\n",
      "     |  \n",
      "     |  vocabulary : Mapping or iterable, optional (default=None)\n",
      "     |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "     |      indices in the feature matrix, or an iterable over terms. If not\n",
      "     |      given, a vocabulary is determined from the input documents.\n",
      "     |  \n",
      "     |  binary : bool (default=False)\n",
      "     |      If True, all non-zero term counts are set to 1. This does not mean\n",
      "     |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "     |      is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      "     |  \n",
      "     |  dtype : type, optional (default=float64)\n",
      "     |      Type of the matrix returned by fit_transform() or transform().\n",
      "     |  \n",
      "     |  norm : 'l1', 'l2' or None, optional (default='l2')\n",
      "     |      Each output row will have unit norm, either:\n",
      "     |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "     |      similarity between two vectors is their dot product when l2 norm has\n",
      "     |      been applied.\n",
      "     |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      "     |      See :func:`preprocessing.normalize`.\n",
      "     |  \n",
      "     |  use_idf : bool (default=True)\n",
      "     |      Enable inverse-document-frequency reweighting.\n",
      "     |  \n",
      "     |  smooth_idf : bool (default=True)\n",
      "     |      Smooth idf weights by adding one to document frequencies, as if an\n",
      "     |      extra document was seen containing every term in the collection\n",
      "     |      exactly once. Prevents zero divisions.\n",
      "     |  \n",
      "     |  sublinear_tf : bool (default=False)\n",
      "     |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  vocabulary_ : dict\n",
      "     |      A mapping of terms to feature indices.\n",
      "     |  \n",
      "     |  fixed_vocabulary_: bool\n",
      "     |      True if a fixed vocabulary of term to indices mapping\n",
      "     |      is provided by the user\n",
      "     |  \n",
      "     |  idf_ : array, shape (n_features)\n",
      "     |      The inverse document frequency (IDF) vector; only defined\n",
      "     |      if ``use_idf`` is True.\n",
      "     |  \n",
      "     |  stop_words_ : set\n",
      "     |      Terms that were ignored because they either:\n",
      "     |  \n",
      "     |        - occurred in too many documents (`max_df`)\n",
      "     |        - occurred in too few documents (`min_df`)\n",
      "     |        - were cut off by feature selection (`max_features`).\n",
      "     |  \n",
      "     |      This is only available if no vocabulary was given.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "     |  \n",
      "     |  TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "     |      matrix of counts.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The ``stop_words_`` attribute can get large and increase the model size\n",
      "     |  when pickling. This attribute is provided only for introspection and can\n",
      "     |  be safely removed using delattr or set to None before pickling.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "     |  >>> corpus = [\n",
      "     |  ...     'This is the first document.',\n",
      "     |  ...     'This document is the second document.',\n",
      "     |  ...     'And this is the third one.',\n",
      "     |  ...     'Is this the first document?',\n",
      "     |  ... ]\n",
      "     |  >>> vectorizer = TfidfVectorizer()\n",
      "     |  >>> X = vectorizer.fit_transform(corpus)\n",
      "     |  >>> print(vectorizer.get_feature_names())\n",
      "     |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "     |  >>> print(X.shape)\n",
      "     |  (4, 9)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TfidfVectorizer\n",
      "     |      CountVectorizer\n",
      "     |      _VectorizerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, raw_documents, y=None)\n",
      "     |      Learn vocabulary and idf from training set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      y : None\n",
      "     |          This parameter is not needed to compute tfidf.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted vectorizer.\n",
      "     |  \n",
      "     |  fit_transform(self, raw_documents, y=None)\n",
      "     |      Learn vocabulary and idf, return term-document matrix.\n",
      "     |      \n",
      "     |      This is equivalent to fit followed by transform, but more efficiently\n",
      "     |      implemented.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      y : None\n",
      "     |          This parameter is ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : sparse matrix, [n_samples, n_features]\n",
      "     |          Tf-idf-weighted document-term matrix.\n",
      "     |  \n",
      "     |  transform(self, raw_documents, copy='deprecated')\n",
      "     |      Transform documents to document-term matrix.\n",
      "     |      \n",
      "     |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      "     |      fit_transform).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      raw_documents : iterable\n",
      "     |          An iterable which yields either str, unicode or file objects.\n",
      "     |      \n",
      "     |      copy : bool, default True\n",
      "     |          Whether to copy X and operate on the copy or perform in-place\n",
      "     |          operations.\n",
      "     |      \n",
      "     |          .. deprecated:: 0.22\n",
      "     |             The `copy` parameter is unused and was deprecated in version\n",
      "     |             0.22 and will be removed in 0.24. This parameter will be\n",
      "     |             ignored.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : sparse matrix, [n_samples, n_features]\n",
      "     |          Tf-idf-weighted document-term matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  idf_\n",
      "     |  \n",
      "     |  norm\n",
      "     |  \n",
      "     |  smooth_idf\n",
      "     |  \n",
      "     |  sublinear_tf\n",
      "     |  \n",
      "     |  use_idf\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CountVectorizer:\n",
      "     |  \n",
      "     |  get_feature_names(self)\n",
      "     |      Array mapping from feature integer indices to feature name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list\n",
      "     |          A list of feature names.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Return terms per document with nonzero entries in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Document-term matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_inv : list of arrays, len = n_samples\n",
      "     |          List of arrays of terms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _VectorizerMixin:\n",
      "     |  \n",
      "     |  build_analyzer(self)\n",
      "     |      Return a callable that handles preprocessing, tokenization\n",
      "     |      and n-grams generation.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      analyzer: callable\n",
      "     |          A function to handle preprocessing, tokenization\n",
      "     |          and n-grams generation.\n",
      "     |  \n",
      "     |  build_preprocessor(self)\n",
      "     |      Return a function to preprocess the text before tokenization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      preprocessor: callable\n",
      "     |            A function to preprocess the text before tokenization.\n",
      "     |  \n",
      "     |  build_tokenizer(self)\n",
      "     |      Return a function that splits a string into a sequence of tokens.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      tokenizer: callable\n",
      "     |            A function to split a string into a sequence of tokens.\n",
      "     |  \n",
      "     |  decode(self, doc)\n",
      "     |      Decode the input into a string of unicode symbols.\n",
      "     |      \n",
      "     |      The decoding strategy depends on the vectorizer parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      doc : str\n",
      "     |          The string to decode.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      doc: str\n",
      "     |          A string of unicode symbols.\n",
      "     |  \n",
      "     |  get_stop_words(self)\n",
      "     |      Build or fetch the effective stop words list.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stop_words: list or None\n",
      "     |              A list of stop words.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _VectorizerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    strip_accents_ascii(s)\n",
      "        Transform accentuated unicode symbols into ascii or nothing\n",
      "        \n",
      "        Warning: this solution is only suited for languages that have a direct\n",
      "        transliteration to ASCII symbols.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        s : string\n",
      "            The string to strip\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        strip_accents_unicode\n",
      "            Remove accentuated char for any unicode symbol.\n",
      "    \n",
      "    strip_accents_unicode(s)\n",
      "        Transform accentuated unicode symbols into their simple counterpart\n",
      "        \n",
      "        Warning: the python-level loop and join operations make this\n",
      "        implementation 20 times slower than the strip_accents_ascii basic\n",
      "        normalization.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        s : string\n",
      "            The string to strip\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        strip_accents_ascii\n",
      "            Remove accentuated char for any unicode symbol that has a direct\n",
      "            ASCII equivalent.\n",
      "    \n",
      "    strip_tags(s)\n",
      "        Basic regexp based HTML / XML tag stripper function\n",
      "        \n",
      "        For serious HTML/XML preprocessing you should rather use an external\n",
      "        library such as lxml or BeautifulSoup.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        s : string\n",
      "            The string to strip\n",
      "\n",
      "DATA\n",
      "    ENGLISH_STOP_WORDS = frozenset({'a', 'about', 'above', 'across', 'afte...\n",
      "    __all__ = ['HashingVectorizer', 'CountVectorizer', 'ENGLISH_STOP_WORDS...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rahul\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('sklearn.feature_extraction.text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function print in module builtins:\n",
      "\n",
      "print(...)\n",
      "    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n",
      "    \n",
      "    Prints the values to a stream, or to sys.stdout by default.\n",
      "    Optional keyword arguments:\n",
      "    file:  a file-like object (stream); defaults to the current sys.stdout.\n",
      "    sep:   string inserted between values, default a space.\n",
      "    end:   string appended after the last value, default a newline.\n",
      "    flush: whether to forcibly flush the stream.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': 'sklearn',\n",
       " '__doc__': '\\nMachine learning module for Python\\n==================================\\n\\nsklearn is a Python module integrating classical machine\\nlearning algorithms in the tightly-knit world of scientific Python\\npackages (numpy, scipy, matplotlib).\\n\\nIt aims to provide simple and efficient solutions to learning problems\\nthat are accessible to everybody and reusable in various contexts:\\nmachine-learning as a versatile tool for science and engineering.\\n\\nSee http://scikit-learn.org for complete documentation.\\n',\n",
       " '__package__': 'sklearn',\n",
       " '__loader__': <_frozen_importlib_external.SourceFileLoader at 0x270e2aee248>,\n",
       " '__spec__': ModuleSpec(name='sklearn', loader=<_frozen_importlib_external.SourceFileLoader object at 0x00000270E2AEE248>, origin='C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\__init__.py', submodule_search_locations=['C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn']),\n",
       " '__path__': ['C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn'],\n",
       " '__file__': 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\__init__.py',\n",
       " '__cached__': 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\__pycache__\\\\__init__.cpython-37.pyc',\n",
       " '__builtins__': {'__name__': 'builtins',\n",
       "  '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\",\n",
       "  '__package__': '',\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>),\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__import__': <function __import__>,\n",
       "  'abs': <function abs(x, /)>,\n",
       "  'all': <function all(iterable, /)>,\n",
       "  'any': <function any(iterable, /)>,\n",
       "  'ascii': <function ascii(obj, /)>,\n",
       "  'bin': <function bin(number, /)>,\n",
       "  'breakpoint': <function breakpoint>,\n",
       "  'callable': <function callable(obj, /)>,\n",
       "  'chr': <function chr(i, /)>,\n",
       "  'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1)>,\n",
       "  'delattr': <function delattr(obj, name, /)>,\n",
       "  'dir': <function dir>,\n",
       "  'divmod': <function divmod(x, y, /)>,\n",
       "  'eval': <function eval(source, globals=None, locals=None, /)>,\n",
       "  'exec': <function exec(source, globals=None, locals=None, /)>,\n",
       "  'format': <function format(value, format_spec='', /)>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals()>,\n",
       "  'hasattr': <function hasattr(obj, name, /)>,\n",
       "  'hash': <function hash(obj, /)>,\n",
       "  'hex': <function hex(number, /)>,\n",
       "  'id': <function id(obj, /)>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x00000270E4FD26C8>>,\n",
       "  'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "  'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "  'iter': <function iter>,\n",
       "  'len': <function len(obj, /)>,\n",
       "  'locals': <function locals()>,\n",
       "  'max': <function max>,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'oct': <function oct(number, /)>,\n",
       "  'ord': <function ord(c, /)>,\n",
       "  'pow': <function pow(x, y, z=None, /)>,\n",
       "  'print': <function print>,\n",
       "  'repr': <function repr(obj, /)>,\n",
       "  'round': <function round(number, ndigits=None)>,\n",
       "  'setattr': <function setattr(obj, name, value, /)>,\n",
       "  'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "  'sum': <function sum(iterable, start=0, /)>,\n",
       "  'vars': <function vars>,\n",
       "  'None': None,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'False': False,\n",
       "  'True': True,\n",
       "  'bool': bool,\n",
       "  'memoryview': memoryview,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'classmethod': classmethod,\n",
       "  'complex': complex,\n",
       "  'dict': dict,\n",
       "  'enumerate': enumerate,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'frozenset': frozenset,\n",
       "  'property': property,\n",
       "  'int': int,\n",
       "  'list': list,\n",
       "  'map': map,\n",
       "  'object': object,\n",
       "  'range': range,\n",
       "  'reversed': reversed,\n",
       "  'set': set,\n",
       "  'slice': slice,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'zip': zip,\n",
       "  '__debug__': True,\n",
       "  'BaseException': BaseException,\n",
       "  'Exception': Exception,\n",
       "  'TypeError': TypeError,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'SystemExit': SystemExit,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'ImportError': ImportError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'OSError': OSError,\n",
       "  'EnvironmentError': OSError,\n",
       "  'IOError': OSError,\n",
       "  'WindowsError': OSError,\n",
       "  'EOFError': EOFError,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'NameError': NameError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'IndentationError': IndentationError,\n",
       "  'TabError': TabError,\n",
       "  'LookupError': LookupError,\n",
       "  'IndexError': IndexError,\n",
       "  'KeyError': KeyError,\n",
       "  'ValueError': ValueError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'ArithmeticError': ArithmeticError,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  'SystemError': SystemError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'BufferError': BufferError,\n",
       "  'Warning': Warning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "  'copyright': Copyright (c) 2001-2019 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "      for supporting Python development.  See www.python.org for more information.,\n",
       "  'license': See https://www.python.org/psf/license/,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  '__IPYTHON__': True,\n",
       "  'display': <function IPython.core.display.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, **kwargs)>,\n",
       "  '__pybind11_internals_v3_msvc__': <capsule object NULL at 0x00000270E5582300>,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x00000270E30E67C8>>},\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " 're': <module 're' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\re.py'>,\n",
       " 'logging': <module 'logging' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\logging\\\\__init__.py'>,\n",
       " 'os': <module 'os' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\os.py'>,\n",
       " '_config': <module 'sklearn._config' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\_config.py'>,\n",
       " 'get_config': <function sklearn._config.get_config()>,\n",
       " 'set_config': <function sklearn._config.set_config(assume_finite=None, working_memory=None, print_changed_only=None)>,\n",
       " 'config_context': <function sklearn._config.config_context(**new_config)>,\n",
       " 'logger': <Logger sklearn (INFO)>,\n",
       " '__version__': '0.22.1',\n",
       " '__SKLEARN_SETUP__': False,\n",
       " '_distributor_init': <module 'sklearn._distributor_init' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\_distributor_init.py'>,\n",
       " '__check_build': <module 'sklearn.__check_build' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\__check_build\\\\__init__.py'>,\n",
       " 'exceptions': <module 'sklearn.exceptions' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\exceptions.py'>,\n",
       " 'externals': <module 'sklearn.externals' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\externals\\\\__init__.py'>,\n",
       " 'utils': <module 'sklearn.utils' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\utils\\\\__init__.py'>,\n",
       " 'base': <module 'sklearn.base' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\base.py'>,\n",
       " 'clone': <function sklearn.base.clone(estimator, safe=True)>,\n",
       " 'show_versions': <function sklearn.utils._show_versions.show_versions()>,\n",
       " '__all__': ['calibration',\n",
       "  'cluster',\n",
       "  'covariance',\n",
       "  'cross_decomposition',\n",
       "  'datasets',\n",
       "  'decomposition',\n",
       "  'dummy',\n",
       "  'ensemble',\n",
       "  'exceptions',\n",
       "  'experimental',\n",
       "  'externals',\n",
       "  'feature_extraction',\n",
       "  'feature_selection',\n",
       "  'gaussian_process',\n",
       "  'inspection',\n",
       "  'isotonic',\n",
       "  'kernel_approximation',\n",
       "  'kernel_ridge',\n",
       "  'linear_model',\n",
       "  'manifold',\n",
       "  'metrics',\n",
       "  'mixture',\n",
       "  'model_selection',\n",
       "  'multiclass',\n",
       "  'multioutput',\n",
       "  'naive_bayes',\n",
       "  'neighbors',\n",
       "  'neural_network',\n",
       "  'pipeline',\n",
       "  'preprocessing',\n",
       "  'random_projection',\n",
       "  'semi_supervised',\n",
       "  'svm',\n",
       "  'tree',\n",
       "  'discriminant_analysis',\n",
       "  'impute',\n",
       "  'compose',\n",
       "  'clone',\n",
       "  'get_config',\n",
       "  'set_config',\n",
       "  'config_context',\n",
       "  'show_versions'],\n",
       " 'setup_module': <function sklearn.setup_module(module)>,\n",
       " 'preprocessing': <module 'sklearn.preprocessing' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\preprocessing\\\\__init__.py'>,\n",
       " 'feature_extraction': <module 'sklearn.feature_extraction' from 'C:\\\\Users\\\\RAHUL\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\feature_extraction\\\\__init__.py'>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__SKLEARN_SETUP__',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__check_build',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_config',\n",
       " '_distributor_init',\n",
       " 'base',\n",
       " 'clone',\n",
       " 'config_context',\n",
       " 'exceptions',\n",
       " 'externals',\n",
       " 'feature_extraction',\n",
       " 'get_config',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'os',\n",
       " 'preprocessing',\n",
       " 're',\n",
       " 'set_config',\n",
       " 'setup_module',\n",
       " 'show_versions',\n",
       " 'sys',\n",
       " 'utils']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function dir in module builtins:\n",
      "\n",
      "dir(...)\n",
      "    dir([object]) -> list of strings\n",
      "    \n",
      "    If called without an argument, return the names in the current scope.\n",
      "    Else, return an alphabetized list of names comprising (some of) the attributes\n",
      "    of the given object, and of attributes reachable from it.\n",
      "    If the object supplies a method named __dir__, it will be used; otherwise\n",
      "    the default dir() logic is used and returns:\n",
      "      for a module object: the module's attributes.\n",
      "      for a class object:  its attributes, and recursively the attributes\n",
      "        of its bases.\n",
      "      for any other object: its attributes, its class's attributes, and\n",
      "        recursively the attributes of its class's base classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get help with Python ```Class```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_sort_features',\n",
       " '_validate_custom_analyzer',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'get_feature_names',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'inverse_transform',\n",
       " 'set_params',\n",
       " 'transform']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dir(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_sort_features',\n",
       " '_validate_custom_analyzer',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_word_ngrams',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'get_feature_names',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'inverse_transform',\n",
       " 'set_params',\n",
       " 'transform']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of methods\n",
    "method_list = [func for func in dir(CountVectorizer) if callable(getattr(CountVectorizer, func))]\n",
    "\n",
    "print(len(dir(CountVectorizer)))\n",
    "print(len(method_list))\n",
    "\n",
    "method_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__dict__', '__doc__', '__module__', '__weakref__', '_white_spaces']\n"
     ]
    }
   ],
   "source": [
    "# above tell we have 56 - 51 = 5 attributes\n",
    "# list them\n",
    "attr = [a for a in dir(CountVectorizer) if not callable(getattr(CountVectorizer, a))]\n",
    "print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_char_ngrams', '_char_wb_ngrams', '_check_stop_words_consistency', '_check_vocabulary', '_count_vocab', '_get_param_names', '_get_tags', '_limit_features', '_more_tags', '_sort_features', '_validate_custom_analyzer', '_validate_params', '_validate_vocabulary', '_warn_for_unused_params', '_word_ngrams', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'fit', 'fit_transform', 'get_feature_names', 'get_params', 'get_stop_words', 'inverse_transform', 'set_params', 'transform']\n"
     ]
    }
   ],
   "source": [
    "# dunder - excluded result\n",
    "methods = [func for func in dir(CountVectorizer) if callable(getattr(CountVectorizer, func)) and not func.startswith('__')]\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For more see: https://stackoverflow.com/questions/1911281/how-do-i-get-list-of-methods-in-a-python-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ```inspect``` module\n",
    "<br/>\n",
    "```import inspect```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
